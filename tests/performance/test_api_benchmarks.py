"""
APIæ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•APIçš„å“åº”æ—¶é—´ã€ååé‡å’Œå¹¶å‘æ€§èƒ½
"""

import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from contextlib import ExitStack
from statistics import mean, median
from unittest.mock import patch

import httpx
import pytest
from fastapi.testclient import TestClient

from football_predict_system.main import app


class APIPerformanceBenchmarks:
    """APIæ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""

    def __init__(self):
        self.client = TestClient(app)
        self.base_url = "http://testserver"

        # æ€§èƒ½åŸºå‡†é˜ˆå€¼
        self.response_time_threshold = 2000  # ms
        self.throughput_threshold = 0.5  # requests/second
        self.max_memory_usage = 100  # MB

    def test_health_endpoint_performance(self):
        """æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹æ€§èƒ½"""
        endpoint = "/health"
        iterations = 100

        # é¢„çƒ­
        for _ in range(10):
            self.client.get(endpoint)

        # æ€§èƒ½æµ‹è¯•
        start_time = time.perf_counter()
        response_times = []

        for _ in range(iterations):
            request_start = time.perf_counter()
            response = self.client.get(endpoint)
            request_end = time.perf_counter()

            assert response.status_code == 200
            response_times.append((request_end - request_start) * 1000)

        end_time = time.perf_counter()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time = (end_time - start_time) * 1000  # ms
        avg_response_time = mean(response_times)
        median_response_time = median(response_times)
        p95_response_time = sorted(response_times)[int(0.95 * len(response_times))]
        throughput = iterations / (total_time / 1000)  # requests/second

        # æ€§èƒ½æ–­è¨€
        assert avg_response_time < self.response_time_threshold, (
            f"Average response time {avg_response_time:.2f}ms exceeds "
            f"threshold {self.response_time_threshold}ms"
        )

        assert throughput > self.throughput_threshold, (
            f"Throughput {throughput:.2f} req/s is below "
            f"threshold {self.throughput_threshold} req/s"
        )

        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        performance_report = {
            "endpoint": endpoint,
            "iterations": iterations,
            "total_time_ms": total_time,
            "avg_response_time_ms": avg_response_time,
            "median_response_time_ms": median_response_time,
            "p95_response_time_ms": p95_response_time,
            "throughput_req_per_sec": throughput,
        }

        print("\nğŸš€ Health Endpoint Performance Report:")
        for key, value in performance_report.items():
            print(
                f"  {key}: {value:.2f}"
                if isinstance(value, float)
                else f"  {key}: {value}"
            )

    def test_prediction_endpoint_performance(self):
        """æµ‹è¯•é¢„æµ‹ç«¯ç‚¹æ€§èƒ½"""
        endpoint = "/api/v1/predict"
        iterations = 50

        request_data = {"home_team": "Liverpool", "away_team": "Manchester City"}

        # é¢„çƒ­
        for _ in range(5):
            self.client.post(endpoint, json=request_data)

        # æ€§èƒ½æµ‹è¯•
        response_times = []

        for _ in range(iterations):
            request_start = time.perf_counter()
            response = self.client.post(endpoint, json=request_data)
            request_end = time.perf_counter()

            # è·³è¿‡é¢„æœŸçš„é”™è¯¯(å› ä¸ºæ²¡æœ‰å®é™…æ¨¡å‹)
            if response.status_code in [200, 500]:
                response_times.append((request_end - request_start) * 1000)

        if response_times:  # å¦‚æœæœ‰æœ‰æ•ˆçš„å“åº”æ—¶é—´
            avg_response_time = mean(response_times)
            p95_response_time = sorted(response_times)[int(0.95 * len(response_times))]

            # é¢„æµ‹ç«¯ç‚¹çš„é˜ˆå€¼æ›´å®½æ¾
            prediction_threshold = 500  # ms

            assert avg_response_time < prediction_threshold, (
                f"Prediction average response time {avg_response_time:.2f}ms "
                f"exceeds threshold {prediction_threshold}ms"
            )

            performance_report = {
                "endpoint": endpoint,
                "iterations": len(response_times),
                "avg_response_time_ms": avg_response_time,
                "p95_response_time_ms": p95_response_time,
            }

            print("\nğŸ¯ Prediction Endpoint Performance Report:")
            for key, value in performance_report.items():
                print(
                    f"  {key}: {value:.2f}"
                    if isinstance(value, float)
                    else f"  {key}: {value}"
                )

    def test_concurrent_requests_performance(self):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚æ€§èƒ½"""
        endpoint = "/health"
        concurrent_users = 10
        requests_per_user = 20

        def make_requests():
            """å•ä¸ªç”¨æˆ·çš„è¯·æ±‚å‡½æ•°"""
            response_times = []
            for _ in range(requests_per_user):
                start = time.perf_counter()
                response = self.client.get(endpoint)
                end = time.perf_counter()

                if response.status_code == 200:
                    response_times.append((end - start) * 1000)
            return response_times

        # å¹¶å‘æµ‹è¯•
        start_time = time.perf_counter()

        patches = [
            patch(
                "football_predict_system.core.health.HealthChecker.check_database_health",
                return_value=(True, "mocked"),
            ),
            patch(
                "football_predict_system.core.health.HealthChecker.check_redis_health",
                return_value=(True, "mocked"),
            ),
            patch(
                "football_predict_system.core.health.HealthChecker.check_model_registry",
                return_value=(True, "mocked"),
            ),
            patch(
                "football_predict_system.core.health.HealthChecker.check_external_apis",
                return_value=(True, "mocked"),
            ),
        ]
        with ExitStack() as stack:
            for p in patches:
                stack.enter_context(p)
            with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
                futures = [
                    executor.submit(make_requests) for _ in range(concurrent_users)
                ]
                all_response_times = []

                for future in futures:
                    user_times = future.result()
                    all_response_times.extend(user_times)

        end_time = time.perf_counter()

        if all_response_times:
            total_requests = len(all_response_times)
            total_time = end_time - start_time
            avg_response_time = mean(all_response_times)
            throughput = total_requests / total_time

            # å¹¶å‘æ€§èƒ½æ–­è¨€
            concurrent_threshold = 2000  # ms for concurrent requests (adjusted for CI)
            concurrent_throughput_threshold = 50  # req/s

            assert avg_response_time < concurrent_threshold, (
                f"Concurrent average response time {avg_response_time:.2f}ms "
                f"exceeds threshold {concurrent_threshold}ms"
            )

            assert throughput > concurrent_throughput_threshold, (
                f"Concurrent throughput {throughput:.2f} req/s is below "
                f"threshold {concurrent_throughput_threshold} req/s"
            )

            performance_report = {
                "concurrent_users": concurrent_users,
                "requests_per_user": requests_per_user,
                "total_requests": total_requests,
                "total_time_s": total_time,
                "avg_response_time_ms": avg_response_time,
                "throughput_req_per_sec": throughput,
            }

            print("\nâš¡ Concurrent Performance Report:")
            for key, value in performance_report.items():
                print(
                    f"  {key}: {value:.2f}"
                    if isinstance(value, float)
                    else f"  {key}: {value}"
                )

    @pytest.mark.asyncio
    async def test_async_client_performance(self):
        """æµ‹è¯•å¼‚æ­¥å®¢æˆ·ç«¯æ€§èƒ½"""
        endpoint = f"{self.base_url}/health"
        concurrent_requests = 50

        async def make_async_request(session: httpx.AsyncClient) -> float:
            """å¼‚æ­¥è¯·æ±‚å‡½æ•°"""
            start = time.perf_counter()
            try:
                response = await session.get(endpoint)
                end = time.perf_counter()
                return (end - start) * 1000 if response.status_code == 200 else 0
            except Exception:
                return 0

        # å¼‚æ­¥å¹¶å‘æµ‹è¯•
        async with httpx.AsyncClient() as client:
            start_time = time.perf_counter()

            tasks = [make_async_request(client) for _ in range(concurrent_requests)]
            response_times = await asyncio.gather(*tasks)

            end_time = time.perf_counter()

        # è¿‡æ»¤æœ‰æ•ˆçš„å“åº”æ—¶é—´
        valid_times = [t for t in response_times if t > 0]

        if valid_times:
            total_time = end_time - start_time
            avg_response_time = mean(valid_times)
            throughput = len(valid_times) / total_time

            # å¼‚æ­¥æ€§èƒ½é€šå¸¸æ›´å¥½
            async_threshold = 150  # ms

            assert avg_response_time < async_threshold, (
                f"Async average response time {avg_response_time:.2f}ms "
                f"exceeds threshold {async_threshold}ms"
            )

            performance_report = {
                "async_concurrent_requests": concurrent_requests,
                "successful_requests": len(valid_times),
                "total_time_s": total_time,
                "avg_response_time_ms": avg_response_time,
                "throughput_req_per_sec": throughput,
            }

            print("\nğŸ”„ Async Performance Report:")
            for key, value in performance_report.items():
                print(
                    f"  {key}: {value:.2f}"
                    if isinstance(value, float)
                    else f"  {key}: {value}"
                )


# æ€§èƒ½æµ‹è¯•å®ä¾‹
benchmarks = APIPerformanceBenchmarks()


def test_health_performance():
    """å¥åº·æ£€æŸ¥æ€§èƒ½æµ‹è¯•"""
    benchmarks.test_health_endpoint_performance()


def test_prediction_performance():
    """é¢„æµ‹æ€§èƒ½æµ‹è¯•"""
    benchmarks.test_prediction_endpoint_performance()


def test_concurrent_performance():
    """å¹¶å‘æ€§èƒ½æµ‹è¯•"""
    benchmarks.test_concurrent_requests_performance()


@pytest.mark.asyncio
async def test_async_performance():
    """å¼‚æ­¥æ€§èƒ½æµ‹è¯•"""
    await benchmarks.test_async_client_performance()


if __name__ == "__main__":
    print("ğŸ Starting Performance Benchmarks...")

    benchmarks.test_health_endpoint_performance()
    benchmarks.test_prediction_endpoint_performance()
    benchmarks.test_concurrent_requests_performance()

    print("\nâœ… Performance benchmarks completed!")
